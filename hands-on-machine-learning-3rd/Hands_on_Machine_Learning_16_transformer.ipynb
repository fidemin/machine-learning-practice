{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99cfd151-e882-4203-8077-4a357b42f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as tl\n",
    "from tensorflow.data import Dataset\n",
    "import tensorflow_datasets as tfds\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e121f02e-ea59-41de-acdf-d31517b70bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMT \n",
    "\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "path = tf.keras.utils.get_file('spa-eng.zip', origin=url, cache_dir='datasets', extract=True)\n",
    "text = (Path(path).with_name('spa-eng') / 'spa.txt').read_text()\n",
    "\n",
    "text = text.replace('¡', '').replace('¿','')\n",
    "pairs = [line.split('\\t') for line in text.splitlines()]\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(pairs)\n",
    "sentences_en, sentences_es = zip(*pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bccac57-dbc5-45fc-85de-dd1426675b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_en = sentences_en[:11_000]\n",
    "sentences_es = sentences_es[:11_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5779d6b5-b097-4aec-8192-9daac1c81417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 10:36:50.803395: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2023-11-30 10:36:50.803426: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2023-11-30 10:36:50.803432: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2023-11-30 10:36:50.803478: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-30 10:36:50.803496: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-11-30 10:36:51.873080: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']\n",
      "['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1000\n",
    "max_length = 50\n",
    "\n",
    "text_vec_layer_en = tl.TextVectorization(vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_es = tl.TextVectorization(vocab_size, output_sequence_length=max_length)\n",
    "\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_es.adapt([f'startofseq {s} endofseq' for s in sentences_es])\n",
    "\n",
    "print(text_vec_layer_en.get_vocabulary()[:10])\n",
    "print(text_vec_layer_es.get_vocabulary()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d9d69d-7639-4cb1-990f-d0279e795d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.constant(sentences_en[:10_000])\n",
    "X_valid = tf.constant(sentences_en[10_000:])\n",
    "\n",
    "X_train_dec = tf.constant([f'startofseq {s}' for s in sentences_es[:10_000]])\n",
    "X_valid_dec = tf.constant([f'startofseq {s}' for s in sentences_es[10_000:]])\n",
    "\n",
    "Y_train = text_vec_layer_es([f'{s} endofseq' for s in sentences_es[:10_000]])\n",
    "Y_valid = text_vec_layer_es([f'{s} endofseq' for s in sentences_es[10_000:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ca18f1-df4b-4492-b8c4-1b841739d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PositionalEncoding(tl.Layer):\n",
    "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        assert embed_size % 2 ==0, 'embed size must be even number'\n",
    "        p, i = np.meshgrid(np.arange(max_length), 2 * np.arange(embed_size // 2))\n",
    "        pos_emb = np.empty((1, max_length, embed_size)) # 1은 사용 시, batch로 값이 들어 왔으르 때 broad casting을 하기 위해 넣어준다. 1 X max_n_sequence X d_emb 라고 보면 된다.\n",
    "        pos_emb[0, :, ::2] = np.sin(p / (10_000 ** (i / embed_size))).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / (10_000 ** (i / embed_size))).T\n",
    "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype)) # numpy.empty는 기본적으로 float64로 만들기 때문에 float32 이하를 tensorflow에서 사용하기 위해서는 type 변경이 필오하다.\n",
    "        self.supports_masking = True # mask propabation for next layer\n",
    "\n",
    "    def call(self, inputs):\n",
    "        n_sequence = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encodings[:, :n_sequence] # shape: (batch, n_sequence, d_emb)\n",
    "\n",
    "\n",
    "def transformer_model(vocab_size, embed_size, max_seq_length, Nx, num_heads, n_units, dropout_rate=0.1):\n",
    "    enc_inputs = tl.Input(shape=[], dtype=tf.string)\n",
    "    dec_inputs = tl.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "    enc_input_ids = text_vec_layer_en(enc_inputs)\n",
    "    dec_input_ids = text_vec_layer_es(dec_inputs)\n",
    "\n",
    "    \n",
    "    # encoding\n",
    "    enc_embedding = tl.Embedding(vocab_size, embed_size, mask_zero=True)(enc_input_ids)\n",
    "    Z = PositionalEncoding(max_seq_length, embed_size)(enc_embedding)\n",
    "    for _ in range(Nx):\n",
    "        skip = Z\n",
    "        attn_layer = tl.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "        Z = attn_layer(Z, value=Z)\n",
    "        Z = tl.LayerNormalization()(tl.Add()([Z, skip]))\n",
    "        skip = Z\n",
    "        Z = tl.Dense(n_units, activation='relu')(Z)\n",
    "        Z = tl.Dense(embed_size)(Z)\n",
    "        Z = tl.Dropout(dropout_rate)(Z)\n",
    "        Z = tl.LayerNormalization()(tl.Add()([Z, skip])) # use Add to propagate mask\n",
    "\n",
    "\n",
    "    # decoding\n",
    "    encoder_outputs = Z\n",
    "    dec_embedding = tl.Embedding(vocab_size, embed_size, mask_zero=True)(dec_input_ids)\n",
    "    Z = PositionalEncoding(max_seq_length, embed_size)(dec_embedding)\n",
    "\n",
    "    for _ in range(Nx):\n",
    "        skip = Z\n",
    "        attn_layer = tl.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "        Z = attn_layer(Z, value=Z, use_causal_mask=True)\n",
    "        Z = tl.LayerNormalization()(tl.Add()([Z, skip])) # use Add to propagate mask\n",
    "        skip = Z\n",
    "        attn_layer = tl.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "        Z = attn_layer(Z, value=encoder_outputs)\n",
    "        Z = tl.LayerNormalization()(tl.Add()([Z, skip]))\n",
    "        skip = Z\n",
    "        Z = tl.Dense(n_units, activation='relu')(Z)\n",
    "        Z = tl.Dense(embed_size)(Z)\n",
    "        Z = tl.LayerNormalization()(tl.Add()([Z, skip]))\n",
    "\n",
    "    Y_proba = tl.Dense(vocab_size, activation='softmax')(Z)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[enc_inputs, dec_inputs], outputs=[Y_proba])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "embed_size = 128\n",
    "n_units = 128\n",
    "max_seq_length = 50\n",
    "num_heads = 2\n",
    "dropout_rate = 0.1\n",
    "\n",
    "\n",
    "\n",
    "model = transformer_model(vocab_size, embed_size, max_seq_length, num_heads, n_units, dropout_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0847ae6-a9a6-4a4e-be17-97e950856d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc4063e3-f984-4dc7-bc0c-99c378957d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.06968934 -0.1425012   0.19917154 -0.07937535 -0.08110089  0.22989309\n",
      "  0.03209518 -0.025309   -0.04700687  0.02382866], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.07010315 -0.1445272   0.20024146 -0.08195658 -0.07090619  0.23147456\n",
      "  0.04827744 -0.03117206 -0.04578125  0.05138787], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.07008444 -0.14992754  0.19950521 -0.08048613 -0.06253864  0.23308489\n",
      "  0.06137362 -0.03492396 -0.04631061  0.07919947], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.01793756 -0.07520054  0.13782263 -0.03132938 -0.10008351  0.18662068\n",
      " -0.02441441 -0.06039337 -0.02696255  0.06822544], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.01793756 -0.07520054  0.13782263 -0.03132938 -0.10008351  0.18662068\n",
      " -0.02441441 -0.06039337 -0.02696255  0.06822544], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.01793756 -0.07520054  0.13782263 -0.03132938 -0.10008351  0.18662068\n",
      " -0.02441441 -0.06039337 -0.02696255  0.06822544], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.01793756 -0.07520054  0.13782263 -0.03132938 -0.10008351  0.18662068\n",
      " -0.02441441 -0.06039337 -0.02696255  0.06822544], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.01793756 -0.07520054  0.13782263 -0.03132938 -0.10008351  0.18662068\n",
      " -0.02441441 -0.06039337 -0.02696255  0.06822544], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.01793756 -0.07520054  0.13782263 -0.03132938 -0.10008351  0.18662068\n",
      " -0.02441441 -0.06039337 -0.02696255  0.06822544], shape=(10,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.01793756 -0.07520054  0.13782263 -0.03132938 -0.10008351  0.18662068\n",
      " -0.02441441 -0.06039337 -0.02696255  0.06822544], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "dec_sample = X_train_dec[:1]\n",
    "dec_input_ids = text_vec_layer_es(dec_sample)\n",
    "dec_embedding = tl.Embedding(vocab_size, embed_size, mask_zero=True)(dec_input_ids)\n",
    "Z = PositionalEncoding(max_seq_length, embed_size)(dec_embedding)\n",
    "attn_layer = tl.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "Z = attn_layer(Z, value=Z, use_causal_mask=True)\n",
    "\n",
    "for i in range(10):\n",
    "    print(Z[0][i][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88b0efb0-2caf-4d47-a095-5c81a0312a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = True\n",
    "if not skip:\n",
    "    history = model.fit((X_train, X_train_dec), Y_train, epochs=1, validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f75f5e0-0a7f-4903-90d0-f3ab43fbebfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9968543648719788}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "result = classifier(['The actors were very convicing'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a07a74ad-37dd-4565-b619-390cc44b3c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d88dab1b554040af114afc0c73c375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "model_name = 'huggingface/distilbert-base-uncased-finetuned-mnli'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70171a12-85ee-4625-be95-d67b28315693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[ 101, 1045, 2066, 4715, 1012,  102, 2057, 2035, 2293, 4715,  999,\n",
       "         102,    0,    0,    0],\n",
       "       [ 101, 3533, 2973, 2005, 1037, 2200, 2146, 2051, 1012,  102, 3533,\n",
       "        2003, 2214, 1012,  102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer(\n",
    "    ['I like soccer. [SEP] We all love soccer!', \n",
    "     'Joe lived for a very long time. [SEP] Joe is old.'],\n",
    "    padding=True, return_tensors='tf')\n",
    "\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e524d6e1-6d91-4a08-be79-aad71eb9a90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[-2.112382  ,  1.1786789 ,  1.410101  ],\n",
       "       [-0.01478315,  1.096246  , -0.99199456]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(token_ids)\n",
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "733fad36-d8d2-4c42-bfeb-f23e9273bc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.01619701, 0.4352357 , 0.5485673 ],\n",
       "       [0.22656   , 0.6881721 , 0.08526792]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_probas = tf.keras.activations.softmax(outputs.logits)\n",
    "Y_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008d1da-5a0e-4120-83f7-9f1f65e86379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional training\n",
    "sentences = [('Sky is blue', 'Sky is red'), ('I love her', 'She loves me')]\n",
    "X_train = tokenizer(sentences, padding=True, return_tensors='tf').data\n",
    "y_train = tf.constant([0, 2]) # contradiction, neutral\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer='nadam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d546d4d-6e90-4710-83a3-0be85d6bcf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 1.2240 - accuracy: 0.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 4.4905 - accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7240a57-457d-48e0-b771-dc5c5ca7edb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
