{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "29dJTzKFuXBv"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as tl\n",
    "from tensorflow.data import Dataset\n",
    "import tensorflow_datasets as tfds\n",
    "from pathlib import Path\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-lHAv3Hufyq",
    "outputId": "b1b9aeda-02f7-4aa7-98dc-c5f28a6d8dd1"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "filepath = tf.keras.utils.get_file('shakespeare.txt', url)\n",
    "\n",
    "with open(filepath) as f:\n",
    "  text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmYT_0AZvU85",
    "outputId": "2a9b4fa1-ddee-47b8-ac0d-7ae94f48b2a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "print(text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Eyx2rIl9vh8c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 11:04:53.650436: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2023-11-29 11:04:53.650455: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2023-11-29 11:04:53.650458: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2023-11-29 11:04:53.650495: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-29 11:04:53.650708: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-11-29 11:04:53.769220: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "text_vec_layer = tl.TextVectorization(split='character', standardize='lower')\n",
    "text_vec_layer.adapt([text])\n",
    "encoded = text_vec_layer([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "I1FrZxkqv86C"
   },
   "outputs": [],
   "source": [
    "n_tokens = text_vec_layer.vocabulary_size()\n",
    "dataset_size = len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1ikvW4GwIm8",
    "outputId": "27c3535d-8363-498c-9128-c72af8579050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3 4 5 6 7]\n",
      " [2 3 4 5 6 7 8]], shape=(2, 7), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 3  4  5  6  7  8  9]\n",
      " [ 4  5  6  7  8  9 10]], shape=(2, 7), dtype=int32)\n",
      "(<tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
      "array([[1, 2, 3, 4, 5, 6],\n",
      "       [2, 3, 4, 5, 6, 7]], dtype=int32)>, <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
      "array([[2, 3, 4, 5, 6, 7],\n",
      "       [3, 4, 5, 6, 7, 8]], dtype=int32)>)\n",
      "(<tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
      "array([[3, 4, 5, 6, 7, 8],\n",
      "       [4, 5, 6, 7, 8, 9]], dtype=int32)>, <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
      "array([[ 4,  5,  6,  7,  8,  9],\n",
      "       [ 5,  6,  7,  8,  9, 10]], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "def to_window(sequence, length):\n",
    "  ds = Dataset.from_tensor_slices(sequence) # character id list로 부터 dataset을 만든다.\n",
    "  ds = ds.window(length + 1, shift=1, drop_remainder=True) # length + 1 size로 한칸씩 오른쪽으로 이동하는 window dataset list의 데이터셋을 만든다.\n",
    "  ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "  return ds\n",
    "\n",
    "\n",
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "  ds = to_window(sequence, length)\n",
    "\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=100_000, seed=seed)\n",
    "\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sample = tf.range(10) + 1\n",
    "\n",
    "\n",
    "sample_ds = to_window(sample, 6)\n",
    "sample_ds = sample_ds.batch(2)\n",
    "for data in sample_ds:\n",
    "  print(data)\n",
    "\n",
    "dataset = to_dataset(sample, 6, batch_size=2)\n",
    "\n",
    "for data in dataset:\n",
    "  print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kFxmCECDxXt9"
   },
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_set = to_dataset(encoded[:100_000], length=length, shuffle=True, seed=42, batch_size=64)\n",
    "valid_set = to_dataset(encoded[100_000:106_000], length=length, batch_size=64)\n",
    "test_set = to_dataset(encoded[106_000:120_000], length=length, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E8Husz961qXm",
    "outputId": "4e5ea917-706f-414c-96a6-2ae07cc1c3a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          656       \n",
      "                                                                 \n",
      " gru (GRU)                   (None, None, 128)         56064     \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 41)          5289      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62009 (242.22 KB)\n",
      "Trainable params: 62009 (242.22 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# test with simple GRU model\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tl.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tl.GRU(128, return_sequences=True),\n",
    "    tl.Dense(n_tokens, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdPfcHpu2NqD",
    "outputId": "0fed34bb-fbf5-4ff7-9aec-9119d8c52ef5"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'checkpoints/my_shakespeare_model', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "skip = True\n",
    "\n",
    "if not skip:\n",
    "    history = model.fit(train_set, validation_data=valid_set, epochs=1, callbacks=[model_ckpt]) # epoch should be at least 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "g8H6Dnns5aFP"
   },
   "outputs": [],
   "source": [
    "generation_model = tf.keras.Sequential(\n",
    "    [\n",
    "        text_vec_layer,\n",
    "        model\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "z4uyA6p92ps0",
    "outputId": "4aeff836-2a6c-422b-bc82-2269ce8465f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 293ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'j'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = generation_model.predict(['to be or not to b'])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)\n",
    "text_vec_layer.get_vocabulary()[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bUHX_dLo7o1f"
   },
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "  pred = generation_model.predict([text])\n",
    "  # print(pred.shape) # 넣은 문자의 각각 다음 문자가 예측되어 나온다. text가 18개면 shape 이 (1, 18, 41) 이다.\n",
    "  y_proba = pred[0, -1:] # 마지막 것만 사용한다.\n",
    "  # print(y_proba.shape)\n",
    "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "  char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0,0]\n",
    "  return text_vec_layer.get_vocabulary()[char_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "U8Z65sWV8uyB"
   },
   "outputs": [],
   "source": [
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "  for _ in range(n_chars):\n",
    "    text += next_char(text, temperature)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXi4QIyO9TKS",
    "outputId": "19994838-cd25-44a0-f747-a07e81734d62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "to be or not to bn,z3ypy pfdt'3j.vpich3jdbr yfzapbkh::cq3j?$ptsb\n"
     ]
    }
   ],
   "source": [
    "print(extend_text('to be or not to b', temperature=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-_-kfTYH9aui"
   },
   "outputs": [],
   "source": [
    "# 감성 분석\n",
    "\n",
    "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
    "    name='imdb_reviews',\n",
    "    split=['train[:90%]', 'train[90%:]', 'test'],\n",
    "    as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QPJeWnvzbD1m"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 11:04:59.676217: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">,\n",
       "  <tf.Tensor: shape=(), dtype=int64, numpy=0>)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(raw_train_set.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_set = raw_train_set.shuffle(5000, seed=42).batch(batch_size).prefetch(1)\n",
    "valid_set = raw_valid_set.batch(batch_size).prefetch(1)\n",
    "test_set = raw_valid_set.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 11:04:59.715840: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "X, y = list(raw_train_set.take(1))[0]\n",
    "print(X)\n",
    "\n",
    "vocab_size = 1000\n",
    "\n",
    "text_vec_layer = tl.TextVectorization(max_tokens=vocab_size)\n",
    "text_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (64,)\n",
      "vectorize result: (64, 926) tf.Tensor(\n",
      "[ 10  42 366 143  36   2   1   1   5   1   1 142 753  15 563   6 161   4\n",
      "   1 913  21   2  18 669   2   1   7  43   1 391  34 772   3   1   1   1\n",
      "   1  30  34 772   3   1   1   5  93  17   4   1   1  13  22  12  10 178\n",
      "   6   1  56 120 132  19  10 209  36   4 385 514  17   4 707 409  71 129\n",
      " 459 138   1   1  21   2   1 151   1 368  87  69 181 332  69  28   1   6\n",
      "  80 136 194 321   1  26 832 952   1   5   1  30   4 146   1   1  23 340\n",
      "   6 543 123  97  79 107  39   2 377  30 263   6   1 219   4 169   5  82\n",
      "   1 888  12 321   1  24   1   1   5 405   1   1 781  24   1  65 517   3\n",
      "   1  43  87  65 120   8  65   1   1   7   1  13   2 146   1   5   2  18\n",
      " 206  43  49   7   1  99  30  43  11   1   1   1   3   1   1  65 517   3\n",
      " 258   6 972  11 843   5   4 394 514   1   1   9  74  30   4   1  58 656\n",
      " 476   7   1   3   9  69 188 168   1   8  12 415 111   1   1 107 480   4\n",
      "   1  16   1   4   1   1   1  24   1  79 107   2 436   1   5   4 211 514\n",
      "   1   1 536   2  88   1  97   6   1  65   1   2   1   5   1   8  34   1\n",
      " 114  23 340  53  71 122   6  28  39   2 377   6  28   1  13  10  39 552\n",
      "   2 224 137 114  47 459  24   1   2   1   5   1   1  39  45  17   2   1\n",
      "  35   1   1   1  26   1  65   1   1   6  65   1  65   1   6 103  43   2\n",
      "   1   5  47   1  13  13   9 227 450   1  15   4  18  19   1   7   1  53\n",
      "  71   1  41   1  49   1   1  24 567   7   4   1   4 640  29  12 227  76\n",
      "   6  83 332  13  13   1   1 245   3 557 524  91 988   1  80   4  51  51\n",
      "   1  18   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0], shape=(926,), dtype=int64)\n",
      "embedding result:  (64, 926, 128) tf.Tensor(\n",
      "[[-0.04280515 -0.01316671  0.02432619 ... -0.02840778 -0.01698784\n",
      "   0.0237375 ]\n",
      " [-0.03644885  0.01496741 -0.04314695 ...  0.02586303  0.00837312\n",
      "   0.03657985]\n",
      " [-0.04968261  0.0160904  -0.01903172 ...  0.0311639  -0.04111062\n",
      "  -0.04561994]\n",
      " ...\n",
      " [-0.01951379 -0.04733765 -0.01102177 ... -0.02291691 -0.03586056\n",
      "  -0.04032941]\n",
      " [-0.01951379 -0.04733765 -0.01102177 ... -0.02291691 -0.03586056\n",
      "  -0.04032941]\n",
      " [-0.01951379 -0.04733765 -0.01102177 ... -0.02291691 -0.03586056\n",
      "  -0.04032941]], shape=(926, 128), dtype=float32)\n",
      "GRU result: (64, 128)\n",
      "final result: (64, 1)\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tl.Embedding(vocab_size, embed_size),\n",
    "    tl.GRU(128),\n",
    "    tl.Dense(1, activation='sigmoid')\n",
    "])\n",
    "X, y = list(train_set.take(1))[0]\n",
    "print('X shape', X.shape)\n",
    "output1 = model.layers[0](X)\n",
    "print('vectorize result:', output1.shape, output1[0])\n",
    "output2 = model.layers[1](output1)\n",
    "print('embedding result: ', output2.shape, output2[0])\n",
    "output3 = model.layers[2](output2)\n",
    "print('GRU result:', output3.shape)\n",
    "output4 = model.layers[3](output3)\n",
    "print('final result:', output4.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "\n",
    "skip = True\n",
    "\n",
    "if not skip:\n",
    "    with tf.device('/cpu:0'):  # using gpu results in very bad performance for this task.\n",
    "        history = model.fit(train_set, validation_data=valid_set, epochs=1)  # result is bad because of 0 vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked version\n",
    "\n",
    "inputs = tl.Input(shape=[], dtype=tf.string)\n",
    "token_ids = text_vec_layer(inputs)\n",
    "mask = tf.math.not_equal(token_ids, 0)\n",
    "Z = tl.Embedding(vocab_size, embed_size)(token_ids)\n",
    "Z = tl.GRU(128, dropout=0.2)(Z, mask=mask)\n",
    "outputs = tl.Dense(1, activation='sigmoid')(Z)\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "\n",
    "skip = True\n",
    "\n",
    "if not skip:\n",
    "    with tf.device('/cpu:0'):  # using gpu results in very bad performance for this task.\n",
    "            history = model.fit(train_set, validation_data=valid_set, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "os.environ['TFHUB_CACHE_DIR'] = 'cache'\n",
    "model = tf.keras.Sequential([\n",
    "    hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder/4',\n",
    "                   trainable=True, dtype=tf.string, input_shape=[]),\n",
    "    tl.Dense(64, activation='relu'),\n",
    "    tl.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = True\n",
    "\n",
    "if not skip:\n",
    "    with tf.device('/cpu:0'):  # using gpu results in very bad performance for this task.\n",
    "        model.fit(train_set, validation_data=valid_set, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "2638744/2638744 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# NMT \n",
    "\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "path = tf.keras.utils.get_file('spa-eng.zip', origin=url, cache_dir='datasets', extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (Path(path).with_name('spa-eng') / 'spa.txt').read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('¡', '').replace('¿','')\n",
    "pairs = [line.split('\\t') for line in text.splitlines()]\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(pairs)\n",
    "sentences_en, sentences_es = zip(*pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How boring! => Qué aburrimiento!\n",
      "I love sports. => Adoro el deporte.\n",
      "Would you like to swap jobs? => Te gustaría que intercambiemos los trabajos?\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f'{sentences_en[i]} => {sentences_es[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "max_length = 50\n",
    "\n",
    "text_vec_layer_en = tl.TextVectorization(vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_es = tl.TextVectorization(vocab_size, output_sequence_length=max_length)\n",
    "\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_es.adapt([f'startofseq {s} endofseq' for s in sentences_es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_en.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_es.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.constant(sentences_en[:100_000])\n",
    "X_valid = tf.constant(sentences_en[100_000:])\n",
    "\n",
    "X_train_dec = tf.constant([f'startofseq {s}' for s in sentences_es[:100_000]])\n",
    "X_valid_dec = tf.constant([f'startofseq {s}' for s in sentences_es[100_000:]])\n",
    "\n",
    "Y_train = text_vec_layer_es([f'{s} endofseq' for s in sentences_es[:100_000]])\n",
    "Y_valid = text_vec_layer_es([f'{s} endofseq' for s in sentences_es[100_000:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000,)\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder state: [<KerasTensor: shape=(None, 512) dtype=float32 (created by layer 'lstm')>, <KerasTensor: shape=(None, 512) dtype=float32 (created by layer 'lstm')>]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)  # 추가 코드 - CPU에서 재현성 보장\n",
    "encoder_inputs = tl.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tl.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "embed_size = 128\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "\n",
    "encoder_embedding_layer = tl.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "decoder_embedding_layer = tl.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n",
    "\n",
    "encoder = tl.LSTM(512, return_state=True)\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "print('encoder state:', encoder_state)\n",
    "\n",
    "decoder = tl.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "output_layer = tl.Dense(vocab_size, activation='softmax')\n",
    "y_proba = output_layer(decoder_outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.legacy.Nadam(\n",
    "    learning_rate=0.05)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 50)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " text_vectorization_2 (Text  (None, 50)                   0         ['input_2[0][0]']             \n",
      " Vectorization)                                                                                   \n",
      "                                                                                                  \n",
      " text_vectorization_3 (Text  (None, 50)                   0         ['input_3[0][0]']             \n",
      " Vectorization)                                                                                   \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)     (None, 50, 128)              128000    ['text_vectorization_2[0][0]']\n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)     (None, 50, 128)              128000    ['text_vectorization_3[0][0]']\n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 512),                1312768   ['embedding_3[0][0]']         \n",
      "                              (None, 512),                                                        \n",
      "                              (None, 512)]                                                        \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, 50, 512)              1312768   ['embedding_4[0][0]',         \n",
      "                                                                     'lstm[0][1]',                \n",
      "                                                                     'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 50, 1000)             513000    ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3394536 (12.95 MB)\n",
      "Trainable params: 3394536 (12.95 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = True\n",
    "\n",
    "if not skip:\n",
    "    model.fit((X_train, X_train_dec), Y_train, epochs=5, validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, sentence_en):\n",
    "    translation = ''\n",
    "    for word_idx in range(max_length):\n",
    "        X = np.array([sentence_en]) # batch 1로 되게끔 감쌈\n",
    "        X_dec = np.array([\"startofseq \" + translation]) # batch 1로 되게끔 감쌈\n",
    "        y_proba = model.predict((X, X_dec))[0, word_idx]\n",
    "        predicted_word_id = np.argmax(y_proba)\n",
    "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == 'endofseq':\n",
    "            break\n",
    "        translation += \" \" + predicted_word\n",
    "    return translation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[UNK] [UNK]'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, 'I like you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 10), dtype=bool, numpy=\n",
       "array([[ True, False, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [ True,  True, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [ True,  True,  True, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [ True,  True,  True,  True, False, False, False, False, False,\n",
       "        False],\n",
       "       [ True,  True,  True,  True,  True, False, False, False, False,\n",
       "        False],\n",
       "       [ True,  True,  True,  True,  True,  True, False, False, False,\n",
       "        False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True]])>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask = tf.linalg.band_part(\n",
    "    tf.ones((10, 10), tf.bool), -1, 0\n",
    ")\n",
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
